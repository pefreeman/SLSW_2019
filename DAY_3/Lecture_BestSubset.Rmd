---
title: "Subset Selection"
author: "Peter Freeman"
date: "Summer 2019"
output: 
  slidy_presentation:
    font_adjustment: -1
---

The Setting
===

We wish to learn a linear model. Our estimate is
$$
\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 X_1 + \cdots + \hat{\beta}_p X_p
$$
where the hats denote estimated quantities.

In subset selection, we attempt to select a subset $s$ out of the $p$ overall
predictors. Why?

1. *To improve prediction accuracy*. Eliminating uninformative predictors can lead to lower variance in the test-set MSE, at the expense of a slight increase in bias.

2. *To improve model interpretability*. Eliminating uninformative predictors is obviously a good thing when your goal is to tell the story of how your predictors are associated with your response.

Note that subset selection is useful and/or necessary if, e.g., $n \lesssim p$ (the sample size is roughly the same as, or less than, the number of predictor variables), but can still be helpful if $n > p$. If $n \gg p$, subset selection generally does not yield useful results.

Best Subset Selection
===

<center>![](http://www.stat.cmu.edu/~pfreeman/Algorithm_6.1.png){width=70%}</center>

(Algorithm 6.1, *Introduction to Statistical Learning* by James et al.)

Note:
$$
{p \choose k} = \frac{p!}{k!(p-k)!}
$$
BSS works for $p \lesssim 25$; otherwise the total number of models is such that lack of computer memory becomes an issue.

BSS Tuning
===

The application of BSS involves tuning: what is the best set of variables to keep? When tuning is involved, we generally have to split the training data into a smaller training set plus a so-called *validation* set, or perform cross-validation on the training set. (For instance, if we originally used 70% of our data to train the model and 30% to assess it, after resplitting the training data we might have 49% [70% of 70%] of our data used for training, 21% [30% of 70%] for validation, and 30% for testing.)

However, here we don't need to explicitly resplit the data: the first three metrics listed under Step 3 above are estimators of the validation set MSE. So we can apply BSS to our full training dataset!

BSS Metrics
===

The functional forms of the metrics given in Step 3 are
\begin{eqnarray*}
C_p &=& \frac{1}{n} ( {\rm RSS} + 2k\hat{\sigma}^2 ) \\
{\rm AIC} &=& \frac{1}{n \hat{\sigma}^2} ( {\rm RSS} + 2k \hat{\sigma}^2 ) = \frac{C_p}{\hat{\sigma}^2} \\
{\rm BIC} &=& \frac{1}{n} ( {\rm RSS} + \log(n)k\hat{\sigma}^2 )
\end{eqnarray*}
RSS denotes the "residual sum-of-squares." The additive terms are penalty terms that increase with $k$ and thus act to prevent overfitting. $\hat{\sigma}^2$ is an estimate of the variance of the linear regression error term $\epsilon$, i.e., the variance of the scatter of data around the regression line (thus the metrics do implicitly assume constant error).

BSS Metrics
===

Typically, $\log(n) > 2$, so BIC (or "Bayesian Information Criterion") imposes a larger penalty relative to $C_p$ (or "Mallow's $C_p$") or AIC (or "Akaike Information Criterion").

$\Rightarrow$ BIC tends to underfit the data (i.e., select as optimal models with fewer variables)

$\Rightarrow$ $C_p$ and AIC tend to overfit the data

Which metric you choose is up to you; the choice should be motivated by your inferential goals.

(What about adjusted $R^2$? The link between that metric and the validation-set MSE is not theoretically well motivated, so one should only use BIC or $C_p$/AIC to select the variable subset.)

BSS: Regression Example
===

```{r}
load(url("https://raw.githubusercontent.com/pefreeman/36-290/master/EXAMPLE_DATASETS/PHOTO_MORPH/photo_morph.Rdata"))
```
The `load()` function places two stored variables into the global environment: <tt>predictors</tt>, a data frame with 16 measurements for each of 3,419 galaxies, and <tt>response</tt>, a vector with 3,419 redshifts (i.e., metrics of physical distance).
```{r}
set.seed(404)
train = sample(nrow(predictors),0.7*nrow(predictors))
pred.train = predictors[train,]
pred.test  = predictors[-train,]
resp.train = response[train]
resp.test  = response[-train]
```

BSS: Regression Example
===

Note: the `leaps` package is only to be used for linear regression. For logistic regression, use `bestglm`.
```{r}
if ( require(leaps) == FALSE ) {
  install.packages("leaps",repos="https://cloud.r-project.org")
  library(leaps)
}
out.reg = regsubsets(resp.train~.,pred.train,nvmax=ncol(pred.train))
summary(out.reg)
```

BSS: Regression Example
===

```{r fig.height=4,fig.width=4,fig.align="center"}
library(ggplot2)
s = summary(out.reg)
df = data.frame(1:16,s$cp,s$bic)
names(df) = c("num.var","Cp","BIC")
ggplot(data=df,mapping=aes(x=num.var,y=Cp)) + geom_point() + geom_line() + ylim(0,100)
coef(out.reg,11)
ggplot(data=df,mapping=aes(x=num.var,y=BIC)) + geom_point() + geom_line() + ylim(-2300,-2100)
```

BSS: Regression Example
===

```{r fig.height=4,fig.width=4,fig.align="center"}
ggplot(data=df,mapping=aes(x=num.var,y=BIC)) + geom_point() + geom_line() + ylim(-2300,-2100)
coef(out.reg,9)
```

BSS: Regression Example
===

One sub-optimal aspect of the `leaps` package is that it does not include a predict function. Here we recreate the `predict.regsubsets()` function that James et al. create for *Introduction to Statistical Learning*:
```{r}
predict.regsubsets = function(object,form,newdata,k) {
  form  = as.formula(form)
  mat   = model.matrix(form,newdata)
  coefi = coef(object,id=k)
  xvars = names(coefi)
  mat[,xvars]%*%coefi
}
resp.pred = predict.regsubsets(out.reg,"resp.test~.",pred.test,9)  # Predictions for BIC
mean((resp.pred-resp.test)^2)
```

BSS: Classification Example
===

The dataset that we read in below contains magnitude and redshift data for 500 quasars and 500 stars. The idea is to learn a classifier that can discriminate between quasars and stars with a low misclassification rate.
```{r}
df = read.csv("https://raw.githubusercontent.com/pefreeman/PSU_2019/master/StarQuasar.csv")
dim(df)
names(df)
names(df)[8] = "y"                                           # necessary tweak for bestglm: response is "y"
set.seed(202)
s = sample(nrow(df),0.7*nrow(df))
data.train = df[s,c(1:5,8)]                                  # don't include redshift or redshift error!
data.test  = df[-s,c(1:5,8)]
```

BSS: Classification Example
===

The `bestglm` package functions are more straightforward to use than `leaps` package functions, in that if you name your metric (e.g., "BIC" or "AIC") ahead of time, it will generate details on the best model for you. However, for logistic regression one is limited to $p \leq 15$ in an exhaustive search (the default) due to computational reasons.
```{r}
if ( require(bestglm) == FALSE ) {
  install.packages("bestglm",repos="http://cloud.r-project.org")
  library(bestglm)
}
out.glm = bestglm(data.train,family=binomial,IC="BIC")
out.glm$BestModel                                      # $Subsets shows results for all k=1,p
```

BSS: Classification Example
===

Like `leaps`, `bestglm` does not include a generic predict function. Here we create our own:
```{r}
predict.bestglm = function(object,data.train,data.test) {
  form  = formula(object$BestModel$terms)
  out.log = glm(form,data=data.train,family=binomial)
  return(predict(out.log,newdata=data.test,type="response"))
}
resp.prob = predict.bestglm(out.glm,data.train,data.test)
resp.pred = ifelse(resp.prob>0.5,"STAR","QSO")
mean(resp.pred!=data.test$y)
table(resp.pred,data.test$y)
```

Forward and Backward Stepwise Selection
===

What if $p \gtrsim 25$, i.e., what if BSS is computationally infeasible? In that case, we might use either *forward* or *backward stepwise selection*. For instance:

<center>![](http://www.stat.cmu.edu/~pfreeman/Algorithm_6.2.png){width=70%}</center>

(Algorithm 6.2, *Introduction to Statistical Learning* by James et al.)

In words, forward stepwise selection starts with no predictor variables and adds one at a time; backward stepwise selection is similar, except that it starts with the full set of predictors and takes one out at a time. One can apply forward and backward stepwise selection using `regsubsets()` or `bestglm()` as above, but with the arguments `method="forward"` or `method="backward"`.

Forward and backward stepwise selection are examples of *greedy algorithms*: they make locally optimally choices that may collectively not yield a globally optimal solution. BSS is always to be preferred, if applying it is computationally feasible.
